{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we got the links from year 2000 to 2024, enough for now\n",
    "# how do we scrape those links one by one\n",
    "\n",
    "# we can use mechanicalsoup for the actual data scraping,\n",
    "# since we dont need to interact with any buttons or such\n",
    "\n",
    "\n",
    "import mechanicalsoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3 import Retry\n",
    "import os\n",
    "import time\n",
    "\n",
    "mbrowser = mechanicalsoup.StatefulBrowser(\n",
    "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36\",\n",
    "    requests_adapters={\n",
    "        \"http\": HTTPAdapter(\n",
    "            max_retries=Retry(total=None, backoff_factor=0.5),\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "#url = \"https://www.imdb.com/title/tt0099685\"\n",
    "#urls = [\n",
    "#    \"https://www.imdb.com/title/tt0099685/\",\n",
    "#    \"https://www.imdb.com/title/tt1130884\",\n",
    "#    \"https://www.imdb.com/title/tt1375666\",\n",
    "#    \"https://www.imdb.com/title/tt0468569\",\n",
    "#]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310610 movies in total\n",
      "289 files\n"
     ]
    }
   ],
   "source": [
    "# get the links from a file\n",
    "#read_links_file = open(\"./imdb_ids/2000-01-02,2000-02-01.txt\", \"r\")\n",
    "\n",
    "# get imdb links (ids) from all files\n",
    "read_links_files = [os.path.join(\"imdb_ids\", file.name) for file in os.scandir(\"imdb_ids\") if file.is_file()]\n",
    "\n",
    "num_total_movies = 0\n",
    "for read_links_file in read_links_files:\n",
    "    read_links_file = open(read_links_file, \"r\", )\n",
    "    links = [link for link in read_links_file.read().split(\"\\n\") if link != \"\"]\n",
    "    num_total_movies += len(links)\n",
    "\n",
    "print(str(num_total_movies) + \" movies in total\")\n",
    "\n",
    "print(str(len(read_links_files)) + \" files\")\n",
    "#print(read_links_files)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# thread safe counter class\n",
    "class ThreadSafeCounter:\n",
    "    def __init__(self) -> None:\n",
    "        self._counter = 0\n",
    "        self._lock = threading.Lock()\n",
    "\n",
    "    def increment(self):\n",
    "        with self._lock:\n",
    "            self._counter += 1\n",
    "    \n",
    "    def value(self):\n",
    "        with self._lock:\n",
    "            return self._counter\n",
    "\n",
    "# TODO: make a chronometer class as well\n",
    "# we can have a seperate thread that keeps track of the time and estimated time maybe?\n",
    "\n",
    "# TODO: DO: make a time tracker class so we can see the estimated time while scraping\n",
    "\n",
    "class ThreadSafeTimer:\n",
    "    def __init__(self) -> None:\n",
    "        self._start_time: float = 0.0\n",
    "        self._stop_time:  float = 0.0\n",
    "\n",
    "        self._is_stopped = False\n",
    "        self._lock = threading.Lock()\n",
    "        \n",
    "        self.start_time()\n",
    "\n",
    "    def start_time(self):\n",
    "        with self._lock:\n",
    "            self._start_time = time.time()\n",
    "            self._is_stopped = False\n",
    "    \n",
    "    def stop_time(self):\n",
    "        with self._lock:\n",
    "            self._stop_time = time.time()\n",
    "            self._is_stopped = True\n",
    "    \n",
    "    def time(self):\n",
    "        with self._lock:\n",
    "            if self._is_stopped == False:\n",
    "                return time.time() - self._start_time\n",
    "            \n",
    "            elif self._is_stopped == True:\n",
    "                return self._stop_time - self._start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "# this huge chunk of code takes the filename that includes imdb ids and scrapes movie data from those links one by one\n",
    "def scrape_movie_data_to_csv(read_links_file_filename, counter_scraped_movies, timer):\n",
    "    read_links_file = open(read_links_file_filename, \"r\", encoding=\"utf-8\")\n",
    "\n",
    "    # links inside the read_links_file file (includes imdb links)\n",
    "    links = [link for link in read_links_file.read().split(\"\\n\") if link != \"\"]\n",
    "\n",
    "    csv_file_name_date = read_links_file_filename.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "    csv_file = open(f\"./csv_files/{csv_file_name_date}.csv\", \"w\", newline=\"\", encoding=\"utf-8\")\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "    csv_writer.writerow([\n",
    "        \"imdb_id\",\n",
    "        \"title\",\n",
    "        \"year\",\n",
    "        \"rating\",\n",
    "        \"time\",\n",
    "        \"imdb_rating\",\n",
    "        \"metascore\",\n",
    "        \"directors\",\n",
    "        \"writers\",\n",
    "        \"stars\"\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    i = 0\n",
    "    for url in links:\n",
    "        #print(f\"{csv_file_name_date}: {i} / {len(links)}\")\n",
    "        #print(f\"{csv_file_name_date}: {i} / {len(links)}\", end=\"\")\n",
    "    \n",
    "        while True:\n",
    "            try:\n",
    "                page = mbrowser.open(\n",
    "                    url,\n",
    "                    timeout=10\n",
    "                )\n",
    "\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print()\n",
    "                print(e)\n",
    "            \n",
    "            \n",
    "        \n",
    "        # original title\n",
    "        #try:\n",
    "        #    title = page.soup.find(\"span\", {\"class\": \"hero__primary-text\"})\n",
    "        #    title = title.text\n",
    "        #except:\n",
    "        #    title = page.soup.find(\"meta\", {\"property\": \"og:title\"})\n",
    "        #    print(title)\n",
    "        #    title = title.content\n",
    "        #    print(title)\n",
    "        title = page.soup.find(\"meta\", {\"property\": \"og:title\"})\n",
    "        #print(title)\n",
    "        title = title.get(\"content\").split(\" (\")[0]\n",
    "\n",
    "        #print(f\"{csv_file_name_date}: {i} / {len(links)} | [{title}] {\" \" * 20}\", end=\"\\r\")\n",
    "        #print(\"+\", end=\"\")\n",
    "            \n",
    "        \n",
    "        ipc_page_sections = page.soup.findAll(\"section\", {\"class\": \"ipc-page-section\"})\n",
    "        container_element = ipc_page_sections[0].findAll(\"div\", recursive=False)[1]\n",
    "        \n",
    "        info_container_element = container_element.findAll(\"div\")[0]\n",
    "        information_elements_list = info_container_element.findAll(\"li\")\n",
    "    \n",
    "        year = information_elements_list[0].text\n",
    "    \n",
    "        # age rating\n",
    "        if len(information_elements_list) < 3: # this means the movie has no age rating\n",
    "            rating = str(-1)\n",
    "        else:\n",
    "            rating = information_elements_list[1].find(\"a\").text\n",
    "        \n",
    "        time = information_elements_list[-1].text\n",
    "            \n",
    "            \n",
    "        #imdb_rating_info_container_element = container_element.findAll(\"div\", recursive=False)[1]\n",
    "        try:\n",
    "            imdb_rating = container_element.find(\"div\", {\"class\": \"rating-bar__base-button\"}) # there are four with the same class name, first one contains imdb score\n",
    "            imdb_rating = imdb_rating.find(\"a\").find(\"span\").text\n",
    "            imdb_rating = imdb_rating.split(\"/\")[0]\n",
    "        except:\n",
    "            imdb_rating = \"null\"\n",
    "        #print(imdb_rating)\n",
    "         \n",
    "        # never use test ids\n",
    "        #imdb_rating = page.soup.findAll(\"div\", attrs={\"data-testid\": \"hero-rating-bar__aggregate-rating__score\"})[0].find(\"span\").text\n",
    "    \n",
    "        # metascore\n",
    "        # there are movies that has no metascore.\n",
    "        try:\n",
    "            metascore = page.soup.findAll(\"span\", {\"class\": \"metacritic-score-box\"})[0].text\n",
    "        except:\n",
    "            metascore = -1 # metascore not found\n",
    "        \n",
    "    \n",
    "        all_li_elements = ipc_page_sections[0].findAll(\"li\", {\"class\": \"ipc-metadata-list__item\"})\n",
    "        \n",
    "        directors = []\n",
    "        writers = []\n",
    "        stars = []\n",
    "        \n",
    "        for li in all_li_elements:\n",
    "            try:\n",
    "                t = li.find(\"span\")\n",
    "                if t is None:\n",
    "                    t = li.find(\"a\")\n",
    "                \n",
    "                t = t.text\n",
    "    \n",
    "                if directors == [] and (t == \"Director\" or t == \"Directors\"):\n",
    "                    items = li.find(\"ul\", {\"class\": \"ipc-inline-list\"}).findAll(\"a\")\n",
    "                    directors = [item.text for item in items]\n",
    "    \n",
    "                elif writers == [] and (t == \"Writer\" or t == \"Writers\"):\n",
    "                    items = li.find(\"ul\", {\"class\": \"ipc-inline-list\"}).findAll(\"a\")\n",
    "                    writers = [item.text for item in items]\n",
    "    \n",
    "                elif stars == [] and (t == \"Star\" or t == \"Stars\"):\n",
    "                    items = li.find(\"ul\", {\"class\": \"ipc-inline-list\"}).findAll(\"a\")\n",
    "                    stars = [item.text for item in items]\n",
    "    \n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "                \n",
    "            \n",
    "        imdb_id = url[27:].split(\"/\")[0]\n",
    "    \n",
    "        \n",
    "        movie_info = [\n",
    "            imdb_id,\n",
    "            title,\n",
    "            str(year),\n",
    "            str(rating),\n",
    "            time,\n",
    "            str(imdb_rating),\n",
    "            str(metascore),\n",
    "            \", \".join(directors),\n",
    "            \", \".join(writers),\n",
    "            \", \".join(stars),\n",
    "        ]\n",
    "    \n",
    "        movie_info_str = \"\\n\".join(movie_info)\n",
    "    \n",
    "        csv_writer.writerow(movie_info)\n",
    "        csv_file.flush()\n",
    "\n",
    "        counter_scraped_movies.increment()\n",
    "        num_scraped_movies = counter_scraped_movies.value()\n",
    "\n",
    "        #print(f\"{num_scraped_movies}/{num_total_movies} ({(num_scraped_movies/num_total_movies) * 100} %) | estimated remaining time: {(num_scraped_movies / num_total_movies) * (time.time() - start_time)} seconds {\" \" * 10}\", end=\"\\r\", flush=True)\n",
    "        estimated_time = (num_total_movies / num_scraped_movies) * timer.time()\n",
    "        h = int(estimated_time // 3600)\n",
    "        m = int((estimated_time % 3600) // 60)\n",
    "        s = int(estimated_time % 60)\n",
    "        print(f\"{num_scraped_movies}/{num_total_movies} ({(num_scraped_movies/num_total_movies) * 100} %) estimated time: {h}:{m}:{s} {\" \" * 10}\", end=\"\\r\", flush=True)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    csv_file.close()\n",
    "\n",
    "#for read_links_file_filename in read_links_files:\n",
    "#    scrape_movie_data_to_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine with 12 cores\n",
      "starting with 12 concurrent threads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/310610 (0.008692572679566015 %) estimated time: 32.0:29.0:46.804292908433126            \r"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "num_cores = os.cpu_count()\n",
    "\n",
    "counter_scraped_movies = ThreadSafeCounter()\n",
    "timer = ThreadSafeTimer()\n",
    "\n",
    "print(f\"machine with {num_cores} cores\")\n",
    "with ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
    "    print(f\"starting with {executor._max_workers} concurrent threads\")\n",
    "    for filename in read_links_files:\n",
    "        executor.submit(scrape_movie_data_to_csv, filename, counter_scraped_movies, timer)\n",
    "\n",
    "print(\"bruh\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
